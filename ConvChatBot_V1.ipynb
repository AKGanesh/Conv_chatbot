{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## V1 - SEQ2SEQ"
      ],
      "metadata": {
        "id": "6fMnimTBVjSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# Load the text data\n",
        "with open(path_to_file, 'r') as f:\n",
        "    text = f.read()\n",
        "\n"
      ],
      "metadata": {
        "id": "70mt_L16cIsR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1daae77e-1e40-4f9b-fe3c-84d3599b33a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts([text])\n",
        "\n",
        "# Convert text to sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences([text])"
      ],
      "metadata": {
        "id": "Pegx7k67ctcL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input-output pairs, incorporating context\n",
        "def create_input_output_pairs(sequences, context_window=3):\n",
        "    input_sequences = []\n",
        "    output_sequences = []\n",
        "    for i in range(context_window, len(sequences)):\n",
        "        input_sequence = sequences[i-context_window:i]\n",
        "        output_sequence = sequences[i]\n",
        "        input_sequences.append(input_sequence)\n",
        "        output_sequences.append(output_sequence)\n",
        "    return input_sequences, output_sequences\n",
        "\n",
        "input_sequences, output_sequences = create_input_output_pairs(sequences)\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_len = 100\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_len)\n",
        "output_sequences = pad_sequences(output_sequences, maxlen=max_len)"
      ],
      "metadata": {
        "id": "XoSaYCCDc-Mk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vocabulary (word_index in older versions of Keras)\n",
        "vocab = tokenizer.word_index\n",
        "idx_enemy=vocab.get('enemy')\n",
        "# Add 1 to account for the 0 index (reserved for padding)\n",
        "oov_index = vocab.get('[UNK]', 1)  # Get the index of '[UNK]' or 1 if not found\n",
        "print(f\"OOV token index: {idx_enemy}\")\n",
        "\n",
        "# Get the word corresponding to the index 186\n",
        "word = list(vocab.keys())[list(vocab.values()).index(88)]\n",
        "print(word)\n",
        "\n",
        "text_test=\"hello world you are the first enemy\"\n",
        "test_sequences = tokenizer.texts_to_sequences([text_test])  # Convert to numerical sequences\n",
        "\n",
        "input_sequences, output_sequences = create_input_output_pairs(test_sequences[0])\n",
        "print(input_sequences)\n",
        "print(output_sequences)\n",
        "\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpvIRa74h5VN",
        "outputId": "90086b95-45a7-4cac-f5b6-d1d9e52d64c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOV token index: 580\n",
            "first\n",
            "[[186, 6, 40], [6, 40, 1], [40, 1, 88]]\n",
            "[1, 88, 580]\n",
            "12632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "\n",
        "def create_seq2seq_model(vocab_size, max_len):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_len,))\n",
        "    encoder_emb = Embedding(vocab_size, 100)(encoder_inputs)\n",
        "    encoder_lstm, state_h, state_c = LSTM(256, return_sequences=True, return_state=True)(encoder_emb)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(1,))  # One-step decoder\n",
        "    decoder_emb = Embedding(vocab_size, 100)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "    # Output Layer\n",
        "    decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "qqT9-8VMdoU7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_seq2seq_model(len(vocab), max_len)\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "M8f_Gc0P63ND"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDlQYUz_p_GD",
        "outputId": "e50e3d24-0c08-47a4-d4c8-ba048cc607f7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True)\n",
        "\n",
        "# ... (tokenization and vocabulary creation)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you have your input_sequences and output_sequences\n",
        "X_train, X_val, y_train, y_val = train_test_split(input_sequences, output_sequences, test_size=0.2, random_state=42)\n",
        "\n",
        "y_train = [[item] for item in y_train]\n",
        "y_val = [[item] for item in y_val]\n",
        "\n",
        "\n",
        "with tf.device(\"/GPU:0\"):\n",
        "  # Tokenize and pad sequences for training and validation sets\n",
        "  encoder_input_data = pad_sequences(X_train, maxlen=max_len, padding='post')\n",
        "  decoder_input_data = pad_sequences(y_train, maxlen=max_len, padding='post')\n",
        "  decoder_target_data = pad_sequences(y_train, maxlen=max_len, padding='post')\n",
        "\n",
        "  val_encoder_input_data = pad_sequences(X_val, maxlen=max_len, padding='post')\n",
        "  val_decoder_input_data = pad_sequences(y_val, maxlen=max_len, padding='post')\n",
        "  val_decoder_target_data = pad_sequences(y_val, maxlen=max_len, padding='post')\n",
        "\n",
        "  model.fit([encoder_input_data, decoder_input_data], decoder_target_data, epochs=10, batch_size=64, validation_data=([val_encoder_input_data, val_decoder_input_data], val_decoder_target_data), callbacks=[early_stop, checkpoint])\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHFyJv1MkpLS",
        "outputId": "ee37f8fa-1ca0-4084-f4ef-1c98ea743c0e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - accuracy: 0.0000e+00 - loss: 9.4453 - val_accuracy: 0.9900 - val_loss: 9.4155\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 542ms/step - accuracy: 0.9900 - loss: 9.4154 - val_accuracy: 0.9900 - val_loss: 9.3760\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 515ms/step - accuracy: 0.9900 - loss: 9.3759 - val_accuracy: 0.9900 - val_loss: 9.3020\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 626ms/step - accuracy: 0.9900 - loss: 9.3017 - val_accuracy: 0.9900 - val_loss: 9.1220\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 618ms/step - accuracy: 0.9900 - loss: 9.1214 - val_accuracy: 0.9900 - val_loss: 8.6604\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 529ms/step - accuracy: 0.9900 - loss: 8.6593 - val_accuracy: 0.9900 - val_loss: 8.0570\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 663ms/step - accuracy: 0.9900 - loss: 8.0545 - val_accuracy: 0.9900 - val_loss: 7.4898\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 526ms/step - accuracy: 0.9900 - loss: 7.4844 - val_accuracy: 0.9900 - val_loss: 6.9741\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 581ms/step - accuracy: 0.9900 - loss: 6.9655 - val_accuracy: 0.9900 - val_loss: 6.4587\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 627ms/step - accuracy: 0.9900 - loss: 6.4470 - val_accuracy: 0.9900 - val_loss: 5.9166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the model\n",
        "model = load_model('./sample_data/best_model.keras')"
      ],
      "metadata": {
        "id": "qujkE1y5n_PI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "861f7991-cd2a-48a4-f5ee-6f8828fa3cc8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "File not found: filepath=./sample_data/best_model.keras. Please ensure the file is an accessible `.keras` zip file.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8719c4f1882f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./sample_data/best_model.keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[1;32m    192\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0;34mf\"File not found: filepath={filepath}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;34m\"Please ensure the file is an accessible `.keras` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: File not found: filepath=./sample_data/best_model.keras. Please ensure the file is an accessible `.keras` zip file."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_text(model, tokenizer, seed_text, max_length, temperature=1.0):\n",
        "    input_seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    input_seq = pad_sequences([input_seq], maxlen=max_len)\n",
        "\n",
        "    # Get the initial decoder state\n",
        "    # This assumes your decoder is an LSTM or GRU layer\n",
        "    # Adjust accordingly for your specific model architecture\n",
        "    decoder_layer = next((layer for layer in model.layers if isinstance(layer, (tf.keras.layers.LSTM, tf.keras.layers.GRU))), None)\n",
        "    if decoder_layer:\n",
        "        decoder_state = [np.zeros((1, decoder_layer.units))]  # Initialize state for the decoder layer\n",
        "    else:\n",
        "        # If no recurrent layer is found, skip state management\n",
        "        decoder_state = None\n",
        "\n",
        "    # Generate text\n",
        "    output_text = []\n",
        "    current_word = seed_text\n",
        "    for _ in range(max_length):\n",
        "        input_seq = tokenizer.texts_to_sequences([current_word])[0]\n",
        "        input_seq = pad_sequences([input_seq], maxlen=1)\n",
        "\n",
        "        # Predict the next word\n",
        "        if decoder_state:\n",
        "            predicted_probs, *new_decoder_state = model.predict([input_seq, decoder_state[0]], verbose=0)\n",
        "            if new_decoder_state:\n",
        "                decoder_state[0] = new_decoder_state[0]\n",
        "        else:\n",
        "            predicted_probs = model.predict([input_seq], verbose=0)\n",
        "\n",
        "        # Apply temperature sampling\n",
        "        predicted_probs = np.asarray(predicted_probs).astype('float64')\n",
        "        predicted_probs = np.log(predicted_probs) / temperature\n",
        "        exp_probs = np.exp(predicted_probs)\n",
        "        predicted_probs = exp_probs / np.sum(exp_probs)\n",
        "        probas = np.random.multinomial(1, predicted_probs[0], 1)\n",
        "        predicted_index = np.argmax(probas)\n",
        "\n",
        "        predicted_word = tokenizer.index_word.get(predicted_index, \"<UNK>\")\n",
        "\n",
        "\n",
        "        output_text.append(predicted_word)\n",
        "        current_word += ' ' + predicted_word\n",
        "\n",
        "    return ' '.join(output_text)"
      ],
      "metadata": {
        "id": "lSC5GwRYntRt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start chatbot\n",
        "context = \"\"\n",
        "while True:\n",
        "    question = input('You: ')\n",
        "    answer = generate_text(model, tokenizer, question, max_len, temperature=0.7)\n",
        "    print('Chatbot:', answer)\n",
        "    # Update context for the next turn\n",
        "    context += f\"You: {question}\\nChatbot: {answer}\\n\""
      ],
      "metadata": {
        "id": "Gk9E5oQAqXAO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "a33237dc-da88-4d01-ed67-8df469bcfdb4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: ji\n",
            "Chatbot: eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids eyelids\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6d2d79a7afd1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Chatbot:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V2 - TRANSFORMER BASED"
      ],
      "metadata": {
        "id": "JLyZRTbxHPKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, PositionalEncoding, TransformerEncoder, TransformerDecoder, Dense\n",
        "\n",
        "# ... (other imports and data preparation)\n",
        "\n",
        "def create_transformer_model(vocab_size, max_len):\n",
        "    # Input layers\n",
        "    encoder_inputs = Input(shape=(max_len,))\n",
        "    decoder_inputs = Input(shape=(max_len,))\n",
        "\n",
        "    # Embedding layers\n",
        "    encoder_embeddings = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "    decoder_embeddings = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
        "\n",
        "    # Positional Encoding\n",
        "    positional_encoding = PositionalEncoding(max_len, embedding_dim)\n",
        "    encoder_inputs = positional_encoding(encoder_embeddings)\n",
        "    decoder_inputs = positional_encoding(decoder_embeddings)\n",
        "\n",
        "    # Encoder\n",
        "    encoder_outputs = TransformerEncoder(num_layers=6, num_heads=8, d_model=embedding_dim, dropout=0.1)(encoder_inputs)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_outputs = TransformerDecoder(num_layers=6, num_heads=8, d_model=embedding_dim, dropout=0.1)(decoder_inputs, encoder_outputs)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder_outputs)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "t-vrEUYuHQsm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "c7638aa8-f8c6-44e6-c289-529a9c1a4fbb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'PositionalEncoding' from 'tensorflow.keras.layers' (/usr/local/lib/python3.10/dist-packages/keras/_tf_keras/keras/layers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-11b018b8782f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPositionalEncoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerDecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ... (other imports and data preparation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'PositionalEncoding' from 'tensorflow.keras.layers' (/usr/local/lib/python3.10/dist-packages/keras/_tf_keras/keras/layers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load pre-trained tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
        "\n",
        "# Prepare your input and target sequences\n",
        "# Assuming 'sequences' contains pairs of input and target sequences\n",
        "input_sequences = [seq[0] for seq in sequences]\n",
        "output_sequences = [seq[1] for seq in sequences]\n",
        "\n",
        "# Convert numerical sequences to text sequences using the tokenizer\n",
        "input_sequences = [tokenizer.decode(seq, skip_special_tokens=True) for seq in input_sequences]  # Decode numerical sequences to text\n",
        "output_sequences = [tokenizer.decode(seq, skip_special_tokens=True) for seq in output_sequences]  # Decode numerical sequences to text\n",
        "\n",
        "\n",
        "# Tokenize input and target sequences\n",
        "input_ids = tokenizer(input_sequences, return_tensors=\"tf\", padding=True, truncation=True)[\"input_ids\"]\n",
        "decoder_input_ids = tokenizer(output_sequences, return_tensors=\"tf\", padding=True, truncation=True)[\"input_ids\"]\n",
        "\n",
        "# Alternatively, if you have target text instead of sequences:\n",
        "# decoder_input_ids = tokenizer(target_text, return_tensors=\"tf\", padding=True, truncation=True)[\"input_ids\"]\n",
        "\n",
        "# Prepare labels by shifting decoder_input_ids and replacing the last token with -100\n",
        "labels = tf.concat([decoder_input_ids[:, 1:], tf.fill([decoder_input_ids.shape[0], 1], -100)], axis=1)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "# Provide both input_ids and decoder_input_ids to the fit method\n",
        "# model.compile(optimizer='adam', loss='categorical_crossentropy') # Changed loss function to 'categorical_crossentropy' as the output is likely categorical\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "# Include labels in the input dictionary\n",
        "model.fit(\n",
        "    {\"input_ids\": input_ids, \"decoder_input_ids\": decoder_input_ids, \"labels\": labels},\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "id": "DukJTHHIHOW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d26f255-0685-40b3-bf90-0e00fe6b5051"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 58s 58s/step - loss: nan\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 127ms/step - loss: nan\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 122ms/step - loss: nan\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 122ms/step - loss: nan\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 116ms/step - loss: nan\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 112ms/step - loss: nan\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 121ms/step - loss: nan\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 110ms/step - loss: nan\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 118ms/step - loss: nan\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 110ms/step - loss: nan\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x7c8094141630>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q0x0mIf6hYdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V3 - PRE-TRAINED"
      ],
      "metadata": {
        "id": "Aksa_xUKWwH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def generate_text_hugg(model, tokenizer, seed_text, max_length, temperature=1.0):\n",
        "    \"\"\"Generates text using the provided model and tokenizer.\n",
        "\n",
        "    Args:\n",
        "        model: The trained language model.\n",
        "        tokenizer: The tokenizer used for encoding and decoding text.\n",
        "        seed_text: The initial text to start generation from.\n",
        "        max_length: The maximum length of the generated text.\n",
        "        temperature: The temperature for sampling.\n",
        "\n",
        "    Returns:\n",
        "        The generated text.\n",
        "    \"\"\"\n",
        "    # Encode the seed text\n",
        "    input_ids = tokenizer(seed_text, return_tensors=\"tf\")[\"input_ids\"]\n",
        "\n",
        "    # Get the initial decoder state (if applicable)\n",
        "    decoder_layer = next((layer for layer in model.layers if isinstance(layer, (tf.keras.layers.LSTM, tf.keras.layers.GRU))), None)\n",
        "    if decoder_layer:\n",
        "        decoder_state = [np.zeros((1, decoder_layer.units))]\n",
        "    else:\n",
        "        decoder_state = None\n",
        "\n",
        "    # Generate text\n",
        "    output_text = []\n",
        "    for _ in range(max_length):\n",
        "        # Predict the next token\n",
        "        if decoder_state:\n",
        "            predicted_probs, *new_decoder_state = model.predict([input_ids, decoder_state[0]], verbose=0)\n",
        "            if new_decoder_state:\n",
        "                decoder_state[0] = new_decoder_state[0]\n",
        "        else:\n",
        "            predicted_probs = model.predict([input_ids], verbose=0)\n",
        "\n",
        "        # Apply temperature sampling\n",
        "        predicted_probs = np.asarray(predicted_probs).astype('float64')\n",
        "        predicted_probs = np.log(predicted_probs) / temperature\n",
        "        exp_probs = np.exp(predicted_probs)\n",
        "        predicted_probs = exp_probs / np.sum(exp_probs)\n",
        "        probas = np.random.multinomial(1, predicted_probs[0], 1)\n",
        "        predicted_index = np.argmax(probas)\n",
        "\n",
        "        # Decode the predicted token\n",
        "        predicted_word = tokenizer.decode(predicted_index, skip_special_tokens=True)\n",
        "\n",
        "        output_text.append(predicted_word)\n",
        "\n",
        "        # Update the input for the next step\n",
        "        input_ids = tf.constant([[predicted_index]]) # Use the predicted token as input for the next step\n",
        "\n",
        "\n",
        "    return ' '.join(output_text)"
      ],
      "metadata": {
        "id": "vzgoZ5UzWyK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start chatbot\n",
        "context = \"\"\n",
        "while True:\n",
        "    question = input('You: ')\n",
        "    answer = generate_text_hugg(model, tokenizer, question, max_len, temperature=0.7)\n",
        "    print('Chatbot:', answer)\n",
        "    # Update context for the next turn\n",
        "    context += f\"You: {question}\\nChatbot: {answer}\\n\""
      ],
      "metadata": {
        "id": "LALc_J_PWzE7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}